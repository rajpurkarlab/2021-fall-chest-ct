experiment_name: 'rsna_window_s3d'
trial_name: 'lr-4'
checkpoint: '/deep/u/alexke/pe_models_benchmark/data/ckpt/rsna_window_s3d_lr-4/1/2022_02_01_22_07_08/epoch=5-step=2069.ckpt'
phase: 'test'
output_dir: '/deep/u/alexke/pe_models_benchmark/data/rsna_window_s3d_lr-4/'

lightning:
  trainer:
    gpus: 4
    distributed_backend: 'ddp' # put on if more than one gpu
    max_epochs: 50
    lr: 1e-4
    precision: 16
    auto_lr_find: false
    benchmark: true
    replace_sampler_ddp: false
    limit_train_batches: 0.1 # for sweep over 10% of data to find best learning rate
  checkpoint_callback:
    monitor: 'val/mean_auroc'
    dirpath: './data/ckpt'
    save_last: true
    mode: 'max'
    save_top_k: 10
  early_stopping_callback:
    monitor: 'val/mean_auroc'
    min_delta: 0.00
    patience: 5
    verbose: False
    mode: 'max'
  logger:
    logger_type: 'WandbLogger'
    save_dir: './data/logger/'
    name: 'coclr'
    project: 'pe_models'


model: 
  type: 'model_3d'
  model_name: 'coclr'
  freeze_cnn: false
  pretrained: false

data: 
  use_hdf5: true
  dataset: 'rsna'
  type: 'window'    # 1d, 2d, 3d, window 
  num_slices: 32
  min_abnormal_slice: 4
  min_positive_slices: 24
  targets: 'rsna_pe_target'
  channels: 'repeat'  # repeat, neighbor, window
  weighted_sample: true
  imsize: 256
    
transforms: 
  type: 'imagenet'
  Rotate:
    rotate_limit: 20
    p: 0.5
  RandomCrop:
    height: 224
    width: 224
  
train: 
  batch_size: 12    # change this with sbatch, 12 fits on one gpu
  num_workers: 8   # change this with sbatch
  weighted_loss: false
  loss_fn: 
    name: 'BCEWithLogitsLoss'
  optimizer: 
    name: 'SGD'
    weight_decay: 1e-3
    momentum: 0.9
    dampening: 0.9
  scheduler: 
    name: 'ReduceLROnPlateau'
    monitor: 'val_loss'
    interval: 'epoch'
    frequency: 3
    factor: 0.5
    patience: 5