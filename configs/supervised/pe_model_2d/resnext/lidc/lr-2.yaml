experiment_name: 'lidc_2d_resnext101'
trial_name: 'lr-2'
phase: 'train'

lightning:
  trainer:
    gpus: 1
    # distributed_backend: 'ddp' # put on if more than one gpu
    max_epochs: 50
    lr: 1e-2
    precision: 16
    auto_lr_find: false
    benchmark: true
    replace_sampler_ddp: false
    limit_train_batches: 0.1 # for sweep over 10% of data to find best learning rate
  checkpoint_callback:
    monitor: 'val/mean_auroc'
    dirpath: './data/ckpt'
    save_last: true
    mode: 'max'
    save_top_k: 10
  early_stopping_callback:
    monitor: 'val/mean_auroc'
    min_delta: 0.00
    patience: 5
    verbose: False
    mode: 'max'
  logger:
    logger_type: 'WandbLogger'
    save_dir: './data/logger/'
    name: 'resnext_101'
    project: 'pe_models'


model: 
  type: 'model_2d'
  model_name: 'resnext_101'
  freeze_cnn: false
  pretrained: true

data: 
  use_hdf5: true
  dataset: 'rsna'
  type: 'lidc-2d'     # 1d, 2d, 3d, window 
  targets: 'rsna_pe_target'
  channels: 'window'   # repeat, neighbor, window
  weighted_sample: true
  positive_only: false
  imsize: 256
    
transforms: 
  type: 'imagenet'
  ShiftScaleRotate:
    shift_limit: 0.05
    scale_limit: 0.05
    rotate_limit: 20
    p: 1.0
  RandomCrop:
    height: 224
    width: 224
  
train: 
  batch_size: 32
  num_workers: 8
  weighted_loss: false
  loss_fn: 
    name: 'BCEWithLogitsLoss'
  optimizer: 
    name: 'Adam'
    weight_decay: 1e-6
  scheduler: 
    name: 'ReduceLROnPlateau'
    monitor: 'val_loss'
    interval: 'epoch'
    frequency: 3
    factor: 0.5
    patience: 5